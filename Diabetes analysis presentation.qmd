---
title: "Group 15 522 Project"
author: "Inder Khera, Jenny Zhang, Jessica Kuo, Javier Martinez"
format: revealjs
jupyter: python3
css: styles.css
---

## Why is Diabetic Study  Important?
- Diabetes is a widespread condition affecting millions globally
- Identifying predictors helps focus on preventive measures for at-risk populations
- Different populations may have unique risk factors
- Reduce healthcare costs and improve the quality of life by preventing long-term complications

---

## Previous Research
- [Use of Neural Networks (Jack W Smith, et. al)](https://pmc.ncbi.nlm.nih.gov/articles/PMC2245318/)
- [Logistic Regression (UCI Machine Learning)](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data)
- [Random Forestion (Quan Zou, et. al)](https://pmc.ncbi.nlm.nih.gov/articles/PMC6232260/)

### What is unique about our study?
- Using Logistic Regression for easier interpretation
- Use of Data Validation and Class Imbalance for better accuracy

---

## Why use a Logistic Regression model?
- Study about predicting whether an individual has diabetes or not
  - Binary Classification
- Provides feature coefficients that directly reflect the importance and direction
- Focis is on probabilities rather than hard classifications
  -  Can help clinicians determine confidence level of a diagnosis

---

## Dataset - Scope
- The dataset contains 768 female patients of Pima Indian heritage, all at least 21 years old.
- This dataset comes from the National Institute of Diabetes and Digestive and Kidney Diseases and was created by selecting instances from a larger medical database.
- The goal: Predict diabetes (Yes/No) based on diagnostic features.
- Available via Kaggle:  
  [Pima Indians Diabetes Database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data).

---

## Dataset - Feature Descriptions

::: {.feature-description}
| **Feature**                 | **Description** |
|-----------------------------|----------------|
| **Pregnancies**             | Number of times the patient has been pregnant. |
| **Glucose**                 | Plasma glucose concentration measured 2 hours after an oral glucose tolerance test. |
| **BloodPressure**           | Diastolic blood pressure (mm Hg). |
| **SkinThickness**           | Triceps skin fold thickness (mm), used to estimate body fat. |
| **Insulin**                 | 2-hour serum insulin level (mu U/ml). |
| **BMI**                     | Body mass index, calculated as weight (kg) / height (m²). |
| **DiabetesPedigreeFunction** | A function that scores the likelihood of diabetes based on family history. |
| **Age**                     | Age of the patient (in years). |
| **Outcome**                 | Target variable: **1 (Diabetic), 0 (Non-Diabetic)**. |
:::

---

## Dataset - Class Imbalance

::: {.class-imbalance}
- The dataset is **imbalanced**, with significantly more **non-diabetic** cases.
- **268 out of 768** patients (**~35%**) have diabetes, while **500 (~65%)** do not.
- This imbalance can impact model training, leading to:
  - A bias toward predicting **non-diabetic cases**.
  - Potential **higher false negatives** (misclassifying diabetic patients as non-diabetic).
:::
![](figures/histogram_imbalance.png)

---

## Data Validation - Pandera
<span style="font-size: 10px;"> <!-- Adjusted the size to 14px -->
![](figures/glucose_outlier.png){width=50%}

- Ensures data integrity: Free of outliers & invalid values
- Based on medically plausible values:
  - Glucose & Blood Pressure can't be 0 if person is alive

![](figures/schema.png){width=70%}
</span>

---

### Data Validation - Deepchecks
:::: {.columns}

::: {.column width="60%"}
![](figures/correlation.png)

![](figures/deepcheck.png)
:::

::: {.column width="40%"}
<br/>

- Correlations close to standard multicollinearity threshold of 0.7

- Use Deepchecks to ensure no multicollinearity is occuring
:::

:::: 

---

## Analysis Methodology4

- Data split: 70% training, 30% testing.
- Features: structured numeric data and no missing values.
- Preprocessing: standardization for easier coefficient interpretation and more stable model performance.

---

## Analysis Methodology

- Accuracy as the evaluation metric for model performance.
- DummyClassifer as baseline model.
- Logistic Regression model for classification.
- Hyperparameter tuning use RandomizedSearchCV to optimize `C` within the range from $10 ^{-5}$ to $10 ^{5}$.

---

## Analysis - Reproducible Data Pipeline

::: {.class-imbalance}
- Our analysis was structured as a **reproducible pipeline**, where each step was **dependent on the previous**.
- We designed **customized functions** to handle:
  - **Data preprocessing**
  - **Feature selection**
  - **Model training**
  - **Evaluation & visualization**
- Each function produced outputs such as:
  - **Plots, tables, and trained models (saved using `pickle`)**
- Functions were **dynamic**, allowing for:
  - Parameter tuning
  - Adjustability based on dependencies
  - Seamless workflow automation
:::

---

## Analysis - Modular Function Design

::: {.class-imbalance}
- The pipeline followed a **modular structure**, ensuring:
  - **Reusability**: Functions could be applied across different datasets.
  - **Interdependency**: Each function received input from a previous step.
  - **Automation**: No manual intervention needed once executed.
  
Example of a function saving model outputs:

```python
import pickle

def save_model(model, filename="logistic_regression.pkl"):
    """Save trained model using pickle for reproducibility."""
    with open(filename, "wb") as f:
        pickle.dump(model, f)
```
:::

---

## Results - Feature Importance

- Feature importance measured by coefficients

![](figures/coeff_table.png)

---

## Results - Model Evaluation

- Dummy Classifier accuracy = 0.672.
- Logistic Regression accuracy:
  - Training set = [0.743] (cross-validation mean)
  - Test set = 0.750

---

## Results - Confusion Matrix

:::: {.columns}

::: {.column width="40%"}
<br/>

#### 217 total test cases

**54 misclassifications**

- *41 false negatives*
 
- *13 false positives*
:::

::: {.column width="60%"}
![](figures/confusion_matrix_plot.png){}
:::

::::

---

## Results - PR and ROC Curve

- Model performance does not achieve optimal trade-off across all thresholds.

:::: {.columns}

::: {.column width="50%"}
![](figures/precision_recall_plot.png)
:::

::: {.column width="50%"}
![](figures/roc_curve.png)
:::

::::

---

## Results - Clinical Utility

- Visualizing predicted probabilities to help clinicians assess model confidence.

![](figures/predict_chart.png){.nostretch fig-align="center" width="750"}

---

## Discussion - Model Performance
- Clinical Relevance: Model shows promise as a screening tool with baseline improvements.
- Enhancement Approaches:
    - Examine 54 misclassified observations and compare with correctly classified examples to identify feature contributions.

---

## Discussion - Enhancement Opportunities
- Explore feature engineering to improve model predictions.
- Experiment with alternative classifiers:
    - Random Forest: Accounts for feature interactions automatically.
    - k-Nearest Neighbours (k-NN): Offers interpretable and decent predictions.
    - Support Vector Classifier (SVC): Enables non-linear prediction using rbf kernel.

---

## Limitations & Future Directions
- Dataset Limitations: Focuses solely on Pima Indian women aged 21 and older, limiting generalizability.
- Future Data Exploration:
    - Collaborate with data collectors for additional, useable information.
    - Combine with external datasets:
        - Broaden demographic coverage (age, gender, ethnicity) 
        - Enable comprehensive insights and greater applicability.

---

## Conclusion - Key Findings
- Logistic regression effectively predicts diabetes among Pima Indian women using features like glucose, BMI, and pregnancies.
- Achieved 0.750 accuracy on the test set, outperforming baseline Dummy Classifier’s 0.672.
- Key predictors: Glucose (most influential) BMI, pregnancies.
- Challenges: 54 misclassifications, including 41 false negatives, highlight risks of undiagnosed cases.

---

## Conclusion - Clinical Implications
- Clinical Potential:
    - Logistic regression shows promise as an initial screening tool for early diabetes detection.
    - Provides a data-driven approach to improve outcomes and reduce complications.

---

## Conclusion - Recommendations
- Recommendations for Improvement:
    - Feature engineering to refine predictors.
    - Test alternative machine learning models 
    - Incorporate additional data (e.g. Lifestyle factors, Genetic information etc.)
    - Include probability estimates to aid clinical decision-making.